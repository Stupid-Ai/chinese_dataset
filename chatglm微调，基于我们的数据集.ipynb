{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b7190e",
   "metadata": {},
   "source": [
    "# 第一步 环境和数据准备\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4133925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#克隆zero_nlp项目\n",
    "!git clone https://github.com/yuanzhoulvpi2017/zero_nlp\n",
    "# 把项目复制到colab对应文件夹\n",
    "!cp -r /content/zero_nlp/simple_thu_chatglm6b/* /content/\n",
    "# 安装环境\n",
    "!pip install protobuf==3.20.0 transformers icetk cpm_kernels peft datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37754081",
   "metadata": {},
   "source": [
    "注意：在上一步结束后，需要检查一下在nodebook的同级文件夹下是否有thuglm这个simple_thu_chatglm6b项目中的文件夹！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b75ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载模型权重\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# 在 /content 目录下创建一个文件夹\n",
    "download_folder = \"/content/thuglm\"\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# 列出要下载的文件路径\n",
    "files_to_download = [\n",
    "    \"config.json\",\n",
    "    \"pytorch_model-00001-of-00008.bin\",\n",
    "    \"pytorch_model-00002-of-00008.bin\",\n",
    "    \"pytorch_model-00003-of-00008.bin\",\n",
    "    \"pytorch_model-00004-of-00008.bin\",\n",
    "    \"pytorch_model-00005-of-00008.bin\",\n",
    "    \"pytorch_model-00006-of-00008.bin\",\n",
    "    \"pytorch_model-00007-of-00008.bin\",\n",
    "    \"pytorch_model-00008-of-00008.bin\",\n",
    "    \"ice_text.model\",\n",
    "]\n",
    "\n",
    "# 下载文件\n",
    "base_url = \"https://huggingface.co/THUDM/chatglm-6b/resolve/main/\"\n",
    "\n",
    "for file_path in files_to_download:\n",
    "    print(f\"Downloading {file_path}...\")\n",
    "    response = requests.get(base_url + file_path)\n",
    "    response.raise_for_status()  # 如果发生错误，抛出异常\n",
    "\n",
    "    # 将文件内容写入本地文件\n",
    "    with open(os.path.join(download_folder, file_path), \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "print(\"All files downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443d14f",
   "metadata": {},
   "source": [
    "# 第二步 数据集转换，把alpaca类型数据集转成chatglm的形式\n",
    "\n",
    "我们感觉可能是数据出问题了，但是不知道哪里出了问题。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e657fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据集转化一下，变成chatglm的格式\n",
    "\n",
    "import json\n",
    "\n",
    "# 读取json文件，这里的json得自己合一下我们的训练数据\n",
    "with open('/content/1.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 将json格式转换为数组对话格式\n",
    "dialogue_data = []\n",
    "for i, d in enumerate(data):\n",
    "    dialogue_data.append('[Round {}]\\n问：{}'.format((1 // 2) + 1, d['instruction']) + '\\n答：{}\\n'.format(d['output']))\n",
    "\n",
    "# 打印结果\n",
    "print(dialogue_data)\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('/content/googdata.csv', mode='w',  encoding='utf-8',newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['content'])\n",
    "    for item in dialogue_data:\n",
    "        writer.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a895979c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'can_return_loss' from 'transformers.utils' (/home/user/anaconda3/envs/codegeexzy/lib/python3.9/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#这个是我从transformers里面复制的Trainer，为chatglm做了适应\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMyTrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForLanguageModeling\n",
      "File \u001b[0;32m/data/MyTrainer.py:137\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    111\u001b[0m     PREFIX_CHECKPOINT_DIR,\n\u001b[1;32m    112\u001b[0m     BestRun,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     speed_metrics,\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptimizerNames, ParallelMode, TrainingArguments\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    138\u001b[0m     CONFIG_NAME,\n\u001b[1;32m    139\u001b[0m     WEIGHTS_INDEX_NAME,\n\u001b[1;32m    140\u001b[0m     WEIGHTS_NAME,\n\u001b[1;32m    141\u001b[0m     can_return_loss,\n\u001b[1;32m    142\u001b[0m     find_labels,\n\u001b[1;32m    143\u001b[0m     get_full_repo_name,\n\u001b[1;32m    144\u001b[0m     is_accelerate_available,\n\u001b[1;32m    145\u001b[0m     is_apex_available,\n\u001b[1;32m    146\u001b[0m     is_datasets_available,\n\u001b[1;32m    147\u001b[0m     is_in_notebook,\n\u001b[1;32m    148\u001b[0m     is_ipex_available,\n\u001b[1;32m    149\u001b[0m     is_sagemaker_dp_enabled,\n\u001b[1;32m    150\u001b[0m     is_sagemaker_mp_enabled,\n\u001b[1;32m    151\u001b[0m     is_torch_compile_available,\n\u001b[1;32m    152\u001b[0m     is_torch_neuroncore_available,\n\u001b[1;32m    153\u001b[0m     is_torch_tpu_available,\n\u001b[1;32m    154\u001b[0m     logging,\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextManagers\n\u001b[1;32m    159\u001b[0m _is_native_cpu_amp_available \u001b[38;5;241m=\u001b[39m is_torch_greater_or_equal_than_1_10\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'can_return_loss' from 'transformers.utils' (/home/user/anaconda3/envs/codegeexzy/lib/python3.9/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "# MyTrainerz在colab上运行的时候164行左右会出问题\n",
    "from MyTrainer import Trainer\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import random\n",
    "from glob import glob\n",
    "from datasets import load_dataset, DatasetDict # 加载数据用的\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# lora已经在peft里面实现了，因此使用peft包即可\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7189c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型和权重\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thuglm\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"thuglm\", trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886042b",
   "metadata": {},
   "source": [
    "# 第三步 使用lora\n",
    "我们这里训练了很多参数，loss也降低了，但是不知道为啥模型还是没反应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf6b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    # ['dense','dense_h_to_4h','dense_4h_to_h'] # 'query_key_value',\n",
    "    target_modules=['query_key_value', ],\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29112fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练和测试数据\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "train_file = \"/content/googdata.csv\"\n",
    "test_file = \"/content/googdata_tes.csv\"\n",
    "\n",
    "raw_datasets = load_dataset(\"csv\", data_files={\n",
    "                            'train': train_file, 'valid': test_file}, cache_dir=\"cache_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4c1da",
   "metadata": {},
   "source": [
    "# 数据处理，我感觉这边会不会有问题.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 512 # 这个大小，基本不影响显存，因此设置为1024也行，目前不知道chatglm要求的文本长度上限为多少\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length <= context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427da56b",
   "metadata": {},
   "source": [
    "# 训练\n",
    "loss降低了，val也挺好，但是就是模型没动静。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a891d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"test003\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=100,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af9d300",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from thuglm.modeling_chatglm import ChatGLMForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc29eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个是我们训练好的模型\n",
    "model = ChatGLMForConditionalGeneration.from_pretrained(\"test003/checkpoint-200\").cuda() #\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"thuglm\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(\"cuda\"):\n",
    "    res, history = model.chat(tokenizer=tokenizer, query=\"今天是星期几？\")\n",
    "        # res = model.forward(input_ids=all_input.get('input_ids').cuda())\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
